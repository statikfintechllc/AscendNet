# AscendNet System Components Integration Guide

## ðŸ§  Core AI Systems

### AscendAI - Core AI Infrastructure
**Location:** `/AscendAI/`
**Purpose:** Foundation AI infrastructure and development framework
**Integration Status:** âœ… Integrated via unified paths

**Key Features:**
- Core AI development framework
- Model registry and management
- Ask Monday AI integration
- Foundational AI utilities

**Integration Points:**
- Configuration: `backend/utils/config.py`
- Path References: `AscendAI/` (maintained as external dependency)
- Access: Via orchestrator unified path system

---

### GremlinGPT - Recursive Autonomous Cognitive System
**Location:** `/AscendAI/GremlinGPT/`
**Purpose:** First R-SRACS (Recursive, Self-Referential Autonomous Cognitive System)
**Integration Status:** âœ… Core logic unified into `backend/ai_core/unified_core.py`

**Key Features:**
- Autonomous decision making and self-mutation
- Multi-agent orchestration and coordination  
- Recursive learning and evolution capabilities
- Memory persistence and context management
- Trading and financial decision systems

**Core Components:**
```
AscendAI/GremlinGPT/
â”œâ”€â”€ agent_core/              # Core autonomous agents
â”œâ”€â”€ agent_shell/             # Agent interaction shells
â”œâ”€â”€ agents/                  # Individual agent implementations
â”œâ”€â”€ self_mutation_watcher/   # Self-improvement monitoring
â”œâ”€â”€ memory/                  # Persistent memory management
â”œâ”€â”€ nlp_engine/              # Natural language processing
â”œâ”€â”€ trading_core/            # Financial decision systems
â”œâ”€â”€ scraper/                 # Data collection systems
â”œâ”€â”€ self_training/           # Autonomous learning
â””â”€â”€ executors/               # Task execution engines
```

**Integration Architecture:**
- **Core Logic:** Integrated into `UnifiedSignalCore` class
- **Autonomous Mode:** Triggered via state evolution
- **Memory System:** Unified with SignalCore memory management
- **Decision Making:** Async processing with mutation functions

---

### GodCore - Quantum Processing & Multi-Model Routing
**Location:** `/GodCore/`
**Purpose:** Advanced AI model routing and quantum-level processing
**Integration Status:** âœ… Core logic unified, Router maintained separately

**Key Features:**
- Multi-model AI routing (Mistral, Monday.AI, GPT, etc.)
- FastAPI-based intelligent request distribution
- Quantum compression and storage management
- Transcendence detection and state management
- Web UI for model interaction

**Core Components:**
```
GodCore/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ router.py           # AI model routing and load balancing
â”‚   â”œâ”€â”€ GPT_handler.py      # GPT integration and management
â”‚   â”œâ”€â”€ MIS_handler.py      # Mistral integration services
â”‚   â”œâ”€â”€ launch_ngrok-UI.sh  # UI deployment automation
â”‚   â””â”€â”€ llama-cpp-python/   # Local LLaMA integration
â”œâ”€â”€ frontend/               # Web UI for model interaction
â”œâ”€â”€ models/                 # Local model storage and management
â””â”€â”€ environment/            # Environment configuration
```

**Integration Architecture:**
- **Quantum Processing:** Integrated into `UnifiedSignalCore._god_core_process()`
- **Model Routing:** Maintained as separate service at `/GodCore/backend/router.py`
- **Storage Compression:** Unified into quantum storage system
- **Transcendence Detection:** Part of state evolution system

**Router Service Integration:**
```python
# GodCore Router runs independently on different port
# Handles model routing between:
MISTRAL_URL = "http://localhost:8000/v1/chat/completions"
MONDAY_URL = "http://localhost:8080/v1/chat/completions"
# Routes requests intelligently based on model capabilities
```

---

### Mobile-Mirror - Secure Mobile Development Environment  
**Location:** `/Mobile-Mirror/`
**Purpose:** Mobile-first development environment with secure remote access
**Integration Status:** ðŸ”„ Planned integration as `backend/mobile_mirror/` service

**Key Features:**
- Mobile-first development environment
- Tailscale mesh VPN integration for secure access
- VSCode server (code-server) deployment
- Zero-config remote development setup
- Private mesh networking (no public IP needed)
- Secure code execution from mobile devices

**Core Components:**
```
Mobile-Mirror/
â”œâ”€â”€ mobilemirror/           # Core mobile mirroring logic
â”œâ”€â”€ scripts/                # Setup and deployment automation
â”œâ”€â”€ env/                    # Environment configuration  
â”œâ”€â”€ demos/                  # Example configurations and use cases
â”œâ”€â”€ docs/                   # Documentation and guides
â””â”€â”€ logs/                   # Service logs and monitoring
```

**Integration Architecture:**
- **Service Layer:** Will be integrated as `backend/mobile_mirror/`
- **API Endpoints:** Mobile development and remote access management
- **Security:** Tailscale mesh VPN integration
- **Development:** Remote VSCode server with secure tunneling

**Planned Integration Endpoints:**
```
/api/v1/mobile/
â”œâ”€â”€ /tunnel/start          # Start secure tunnel
â”œâ”€â”€ /tunnel/status         # Check tunnel status  
â”œâ”€â”€ /dev/environment       # Setup development environment
â”œâ”€â”€ /dev/sync             # Sync code between devices
â””â”€â”€ /security/mesh        # Manage mesh network
```

---

## ðŸ”— Integration Status Summary

| Component | Location | Integration | Status |
|-----------|----------|-------------|---------|
| **AscendAI** | `/AscendAI/` | External Dependency | âœ… Complete |
| **GremlinGPT** | `/AscendAI/GremlinGPT/` | Core Logic Unified | âœ… Complete |
| **GodCore** | `/GodCore/` | Hybrid (Core + Service) | âœ… Complete |
| **SignalCore** | `backend/ai_core/unified_core.py` | Fully Unified | âœ… Complete |
| **Mobile-Mirror** | `/Mobile-Mirror/` | Planned Service | ðŸ”„ Pending |

## ðŸš€ Usage Examples

### Accessing GremlinGPT Autonomous Mode
```python
from backend.ai_core.unified_core import AscendNetAICore

ai_core = AscendNetAICore(config)
await ai_core.initialize()

# Trigger autonomous mode
await ai_core.process_signal("error: system overload detected")
# GremlinGPT automatically activates autonomous decision making
```

### Using GodCore Model Routing
```bash
# Start GodCore router service
cd /GodCore/backend
python router.py --port 8002

# Route requests to best available model
curl -X POST http://localhost:8002/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "auto", "messages": [{"role": "user", "content": "Hello"}]}'
```

### Mobile Development Access (Planned)
```bash
# Setup mobile development environment
curl -X POST http://localhost:8000/api/v1/mobile/dev/environment \
  -H "Authorization: Bearer <token>" \
  -d '{"device": "mobile", "environment": "python"}'

# Start secure tunnel
curl -X POST http://localhost:8000/api/v1/mobile/tunnel/start \
  -H "Authorization: Bearer <token>"
```

## ðŸ”§ Configuration Integration

All components are configured through the unified configuration system:

```json
{
  "ai_core": {
    "ascend_ai_enabled": true,
    "gremlin_gpt_enabled": true,
    "god_core_enabled": true,
    "mobile_mirror_enabled": true,
    "signal_core_enabled": true
  }
}
```

This integration provides a seamless, unified experience while maintaining the unique capabilities of each component.
